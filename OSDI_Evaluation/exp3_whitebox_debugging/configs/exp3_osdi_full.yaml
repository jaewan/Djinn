# Experiment 3: White-Box Interactivity (OSDI Major Revision)
# Scientific evaluation with Llama-2-13B and full baselines

model:
  name: "mistralai/Mistral-7B-v0.1"
  num_layers: 32
  source: "transformers"
  
experiment:
  num_trials: 3
  
  breakpoints:
    enabled: true
    # Test early, mid, and late layers to show scalability
    layers: [8, 16, 24]  # Early, mid, late layers of 32-layer Mistral-7B
  
  inference:
    input_length: 512
    context_tokens: 512
    token_accuracy_threshold: 95.0
    
  activation_steering:
    enabled: false  # TODO: Debug Mistral layer unpacking in resume_from_checkpoint
    modification_type: "scale"  # Scale hidden states
    modification_factor: 0.9    # Multiply by 0.9 to "steer"
    steering_layer: 16          # Modify at mid-point layer (for Mistral-7B)
    
  concurrent_demo:
    enabled: true
    request_b_during_pause: true
    pause_duration_seconds: 5
    
validation:
  require_output_equivalence: true
  require_steering_effect: true
  require_concurrent_success: true
  
duration: 180  # minutes (estimated for full evaluation with baselines)
output_dir: "/tmp/exp3_osdi_results"
