# Experiment 3: White-Box Interactivity (OSDI Major Revision)
# Scientific evaluation with Llama-2-13B and full baselines

model:
  name: "gpt2"
  num_layers: 12
  source: "transformers"
  
experiment:
  num_trials: 3
  
  breakpoints:
    enabled: true
    # Test early, mid, and late layers to show scalability
    layers: [3, 6, 9]  # Early, mid, late layers of 12-layer GPT-2
  
  inference:
    input_length: 512
    context_tokens: 512
    token_accuracy_threshold: 95.0
    
  activation_steering:
    enabled: true  # âœ… ENABLED: Activation steering demo on GPT-2
    modification_type: "scale"  # Scale hidden states
    modification_factor: 0.9    # Multiply by 0.9 to "steer"
    steering_layer: 6           # Modify at mid-point layer (GPT-2 layer 6 of 12)
    
  concurrent_demo:
    enabled: true
    request_b_during_pause: true
    pause_duration_seconds: 5
    
validation:
  require_output_equivalence: true
  require_steering_effect: true
  require_concurrent_success: true
  
duration: 180  # minutes (estimated for full evaluation with baselines)
output_dir: "/tmp/exp3_osdi_results"

