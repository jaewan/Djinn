# Experiment 3: White-Box Breakpoint Debugging - Full Evaluation
# Comprehensive breakpoint debugging evaluation with statistical rigor

model:
  name: "gpt2"  # 12 layers (switch to gpt2-xl for 48 layers on H100)
  source: "transformers"
  
experiment:
  num_trials: 3  # Run 3 trials per layer for statistical significance
  
  breakpoints:
    enabled: true
    # Sweep breakpoint positions to show overhead scales with activation size
    layers: [3, 6, 9]  # Valid for GPT-2's 12 layers
  
  inference:
    input_length: 128
    token_accuracy_threshold: 95.0  # Accept 5% token mismatch due to FP16 effects
    
validation:
  require_output_equivalence: true  # Strict: breakpoint run must == full run
  
duration: 30  # minutes (estimated for 3 trials on 4 layers)
output_dir: "/tmp/exp3_results"

