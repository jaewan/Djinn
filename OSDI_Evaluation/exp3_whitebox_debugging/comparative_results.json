{
  "timestamp": "2025-12-08 00:29:00",
  "baselines": {
    "pytorch_eager": {
      "status": "error",
      "baseline": "pytorch_eager",
      "error": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
    },
    "vllm": {
      "status": "success",
      "baseline": "vllm",
      "model": "gpt2",
      "api_tests": {},
      "conclusion": "",
      "vllm_initialized": false
    },
    "djinn": {
      "status": "success",
      "token_accuracy": {
        "entries": [
          {
            "trial": 1,
            "layer": 3,
            "token_accuracy": 100.0,
            "latency_ms": 37503.36051599993,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 1.5730119999943781,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.004194322797615135
          },
          {
            "trial": 1,
            "layer": 6,
            "token_accuracy": 100.0,
            "latency_ms": 718.4501499999669,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 1.688244000092709,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.2349841530540131
          },
          {
            "trial": 1,
            "layer": 9,
            "token_accuracy": 100.0,
            "latency_ms": 718.9019190000181,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 1.7172849999269602,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.2388761184996805
          },
          {
            "trial": 2,
            "layer": 3,
            "token_accuracy": 100.0,
            "latency_ms": 709.6902890000365,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 1.6530450000118435,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.23292484420788778
          },
          {
            "trial": 2,
            "layer": 6,
            "token_accuracy": 100.0,
            "latency_ms": 520.9256820000974,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 0.8875680000528519,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.17038284552319038
          },
          {
            "trial": 2,
            "layer": 9,
            "token_accuracy": 100.0,
            "latency_ms": 510.7574810000415,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 1.1637540000037916,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.22784864506051103
          },
          {
            "trial": 3,
            "layer": 3,
            "token_accuracy": 100.0,
            "latency_ms": 515.9459579999748,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 1.0319109999272769,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.20000369882291574
          },
          {
            "trial": 3,
            "layer": 6,
            "token_accuracy": 100.0,
            "latency_ms": 510.72160000001077,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 0.9298689999468479,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.18206964419496421
          },
          {
            "trial": 3,
            "layer": 9,
            "token_accuracy": 100.0,
            "latency_ms": 514.7975840000072,
            "checkpoint_time_ms": 0.0,
            "restore_time_ms": 1.0654019999947195,
            "checkpoint_size_mb": 2.503908157348633,
            "os_overhead_percent": 0.20695551671328447
          }
        ],
        "token_accuracy_mean": 100.0,
        "token_accuracy_std": 0.0,
        "latency_mean_ms": 4691.505686555564,
        "latency_std_ms": 11601.106080078129,
        "os_overhead_mean_percent": 0.18869330987489583,
        "os_overhead_max_percent": 0.2388761184996805
      },
      "activation_steering": {
        "layer": 6,
        "scale": 0.9,
        "output_changed": true,
        "baseline_restore_time_ms": 0.7548959999894578,
        "steered_restore_time_ms": 0.7042139999384744,
        "token_diff_percent": 0.390625,
        "steering_successful": true
      },
      "concurrent_demo": {
        "request_b_latency_ms": 374.4050130000005,
        "vram_before_pause_gb": 41.32452392578125,
        "vram_during_pause_gb": 41.45733642578125,
        "resume_restore_time_ms": 0.9003779999829931,
        "request_b_tokens": 512
      },
      "baseline_prompt": "The future of AI is context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token context token"
    }
  },
  "summary_table": {
    "breakpoint_support": {
      "pytorch_eager": "Manual (holds VRAM)",
      "vllm": "NOT POSSIBLE",
      "djinn": "Native pause/resume"
    },
    "vram_during_pause_gb": {
      "pytorch_eager": "N/A",
      "vllm": "N/A",
      "djinn": "41.46 GB"
    },
    "concurrent_requests": {
      "pytorch_eager": "NO",
      "vllm": "NO",
      "djinn": "YES"
    },
    "activation_steering": {
      "pytorch_eager": "Manual, blocks GPU",
      "vllm": "NOT POSSIBLE",
      "djinn": "Steering demo completed"
    },
    "token_accuracy_percent": {
      "pytorch_eager": "100 (reference)",
      "vllm": "N/A",
      "djinn": "100.00"
    }
  }
}