#!/usr/bin/env python3
"""
Phase 4 Validation: Logit Equivalence Test

Verifies that ring buffer produces identical outputs to standard PyTorch.

Pass Condition:
- torch.norm(djinn_output - pytorch_output) < 0.1 (FP16 tolerance)
- Next-token predictions match exactly (backup criterion)

From evaluation plan: "Run one pass with standard PyTorch (loading model fully on CPU or 2 GPUs).
Run one pass with Djinn (Ring Buffer). Pass Condition: torch.norm(djinn_output - ref_output) < 0.1"
"""

import argparse
import json
import logging
import time
import torch
import sys
from pathlib import Path
from typing import Tuple, Dict, Any, Optional

# Add project root
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from transformers import AutoTokenizer, AutoModelForCausalLM

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def compare_tensors(
    tensor1: torch.Tensor,
    tensor2: torch.Tensor,
    rtol: float = 1e-2,
    atol: float = 0.02,
    name: str = "tensor"
) -> Tuple[bool, str, Dict[str, float]]:
    """
    Compare two tensors for numerical equality.
    
    Args:
        tensor1: First tensor (Ring buffer result)
        tensor2: Second tensor (PyTorch reference)
        rtol: Relative tolerance
        atol: Absolute tolerance
        name: Name for error messages
    
    Returns:
        (is_match, error_message, metrics)
    """
    metrics = {}
    
    try:
        # Check shapes
        if tensor1.shape != tensor2.shape:
            return False, f"{name} shape mismatch: {tensor1.shape} vs {tensor2.shape}", metrics
        
        # Handle dtype differences
        if tensor1.dtype != tensor2.dtype:
            if tensor1.dtype == torch.float16 and tensor2.dtype == torch.float32:
                tensor2 = tensor2.half()
            elif tensor1.dtype == torch.float32 and tensor2.dtype == torch.float16:
                tensor1 = tensor1.half()
            else:
                return False, f"{name} dtype mismatch: {tensor1.dtype} vs {tensor2.dtype}", metrics
        
        # Calculate difference metrics
        diff = torch.abs(tensor1 - tensor2)
        max_diff = torch.max(diff).item()
        mean_diff = torch.mean(diff).item()
        norm_diff = torch.norm(tensor1 - tensor2).item()
        
        metrics["max_diff"] = max_diff
        metrics["mean_diff"] = mean_diff
        metrics["norm_diff"] = norm_diff
        
        # Check numerical equality
        if torch.allclose(tensor1, tensor2, rtol=rtol, atol=atol):
            msg = f"Match (norm_diff={norm_diff:.4f}, max_diff={max_diff:.4f}, mean_diff={mean_diff:.4f})"
            return True, msg, metrics
        else:
            msg = (
                f"Mismatch (norm_diff={norm_diff:.4f}, max_diff={max_diff:.4f}, "
                f"mean_diff={mean_diff:.4f}) [rtol={rtol}, atol={atol}]"
            )
            return False, msg, metrics
    
    except Exception as e:
        return False, f"{name} comparison failed: {str(e)}", metrics


def get_next_tokens(logits: torch.Tensor, top_k: int = 5) -> torch.Tensor:
    """
    Get top-k next token predictions from logits.
    
    Args:
        logits: Shape (batch, seq_len, vocab_size)
        top_k: Number of top tokens to extract
    
    Returns:
        Shape (batch, top_k) - indices of top-k tokens at final position
    """
    final_logits = logits[:, -1, :]  # Last token position
    _, top_indices = torch.topk(final_logits, top_k, dim=-1)
    return top_indices


def test_vanilla_pytorch(
    model,
    tokenizer,
    prompts: list[str],
    device: torch.device
) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """
    Run inference with vanilla PyTorch (ground truth).
    
    Returns:
        (logits_tensor, metrics)
    """
    logger.info("\n" + "=" * 80)
    logger.info("VANILLA PYTORCH INFERENCE (Ground Truth)")
    logger.info("=" * 80)
    
    model = model.to(device)
    model.eval()
    
    all_logits = []
    
    with torch.no_grad():
        for prompt in prompts:
            # Tokenize
            inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(device)
            
            # Warmup
            for _ in range(2):
                _ = model(**inputs, use_cache=False)
            if device.type == 'cuda':
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
            
            # Run inference
            start = time.perf_counter()
            outputs = model(**inputs, use_cache=False)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            elapsed = (time.perf_counter() - start) * 1000
            
            logits = outputs.logits.cpu()
            all_logits.append(logits)
            
            logger.info(f"  Prompt: {prompt[:60]}...")
            logger.info(f"  Logits shape: {logits.shape}, Time: {elapsed:.2f}ms")
    
    # Concatenate along batch dimension if multiple prompts
    if len(all_logits) > 1:
        combined_logits = torch.cat(all_logits, dim=0)
    else:
        combined_logits = all_logits[0]
    
    metrics = {
        "inference_time_ms": elapsed,
        "num_prompts": len(prompts),
        "logits_shape": tuple(combined_logits.shape),
    }
    
    logger.info(f"‚úÖ Vanilla PyTorch inference completed")
    logger.info(f"   Combined logits shape: {combined_logits.shape}")
    
    return combined_logits, metrics


def test_ring_buffer(
    model,
    tokenizer,
    prompts: list[str],
    device: torch.device = torch.device("cuda:0")
) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """
    Run inference with ring buffer.
    
    Returns:
        (logits_tensor, metrics)
    """
    logger.info("\n" + "=" * 80)
    logger.info("RING BUFFER INFERENCE")
    logger.info("=" * 80)
    
    try:
        from djinn.backend.runtime.ring_buffer import WeightRingBuffer
        from djinn.backend.runtime.weight_streamer import WeightStreamer
        from djinn.backend.runtime.weight_hooks import install_ring_buffer_hooks
    except ImportError as e:
        logger.error(f"Failed to import ring buffer components: {e}")
        logger.error("Ring buffer implementation may not be complete. Skipping test.")
        return None, {"error": str(e)}
    
    # Calculate model size
    state_dict = model.state_dict()
    model_size_bytes = sum(t.numel() * t.element_size() for t in state_dict.values())
    model_size_gb = model_size_bytes / (1024**3)
    
    logger.info(f"Model size: {model_size_gb:.1f}GB")
    
    # Create ring buffer (1.5x model size for safety)
    ring_capacity_bytes = int(model_size_bytes * 1.5)
    
    try:
        ring_buffer = WeightRingBuffer(ring_capacity_bytes, device=device)
        ring_buffer.register_model("test_model", state_dict)
        logger.info(f"‚úÖ Ring buffer created and registered: {ring_capacity_bytes / 1024**3:.1f}GB")
    except Exception as e:
        logger.error(f"Failed to create ring buffer: {e}")
        return None, {"error": f"Ring buffer creation failed: {str(e)}"}
    
    # Create weight streamer
    try:
        streamer = WeightStreamer(ring_buffer, device=device)
        streamer.start()
        logger.info("‚úÖ Weight streamer started")
    except Exception as e:
        logger.error(f"Failed to start weight streamer: {e}")
        return None, {"error": f"Weight streamer startup failed: {str(e)}"}
    
    # Install hooks (weights already registered with ring buffer above)
    try:
        install_ring_buffer_hooks(model, ring_buffer, "test_model", streamer)
        logger.info("‚úÖ Ring buffer hooks installed")
    except Exception as e:
        logger.error(f"Failed to install hooks: {e}")
        return None, {"error": f"Hook installation failed: {str(e)}"}
    
    # Run inference
    all_logits = []
    
    model.eval()
    model = model.to(device)
    
    with torch.no_grad():
        for prompt in prompts:
            inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(device)
            
            # Warmup
            for _ in range(2):
                _ = model(**inputs, use_cache=False)
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            
            # Run inference
            start = time.perf_counter()
            outputs = model(**inputs, use_cache=False)
            torch.cuda.synchronize()
            elapsed = (time.perf_counter() - start) * 1000
            
            logits = outputs.logits.cpu()
            all_logits.append(logits)
            
            logger.info(f"  Prompt: {prompt[:60]}...")
            logger.info(f"  Logits shape: {logits.shape}, Time: {elapsed:.2f}ms")
    
    # Concatenate results
    if len(all_logits) > 1:
        combined_logits = torch.cat(all_logits, dim=0)
    else:
        combined_logits = all_logits[0]
    
    metrics = {
        "inference_time_ms": elapsed,
        "num_prompts": len(prompts),
        "logits_shape": tuple(combined_logits.shape),
    }
    
    logger.info(f"‚úÖ Ring buffer inference completed")
    logger.info(f"   Combined logits shape: {combined_logits.shape}")
    
    streamer.stop()
    
    return combined_logits, metrics


def main():
    """Main test function."""
    parser = argparse.ArgumentParser(description="Phase 4: Logit Equivalence Test")
    parser.add_argument('--model', type=str, default='meta-llama/Llama-2-7b-hf',
                       help='Model ID from HuggingFace')
    parser.add_argument('--num-samples', type=int, default=5,
                       help='Number of test samples')
    parser.add_argument('--output-dir', type=str, default='OSDI_Evaluation/phase4_validation/results',
                       help='Output directory for results')
    parser.add_argument('--skip-ring-buffer', action='store_true',
                       help='Skip ring buffer test (test PyTorch only)')
    
    args = parser.parse_args()
    
    # Check CUDA
    if not torch.cuda.is_available():
        logger.error("‚ùå CUDA not available")
        return False
    
    device = torch.device("cuda:0")
    logger.info(f"‚úÖ Using device: {device}")
    logger.info(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # Load model
    logger.info(f"\nüîÑ Loading model: {args.model}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model)
        model = AutoModelForCausalLM.from_pretrained(
            args.model,
            torch_dtype=torch.float16,
            device_map="cpu"
        )
        logger.info(f"‚úÖ Model loaded: {model.config.model_type}")
    except Exception as e:
        logger.error(f"‚ùå Failed to load model: {e}")
        return False
    
    # Test prompts
    test_prompts = [
        "The future of artificial intelligence is",
        "Machine learning enables computers to",
        "Deep neural networks can process",
        "Transformer models revolutionized",
        "Semantic understanding in AI requires",
    ][:args.num_samples]
    
    logger.info(f"\nüìã Test prompts ({len(test_prompts)}):")
    for i, p in enumerate(test_prompts, 1):
        logger.info(f"  {i}. {p}")
    
    # Test 1: Vanilla PyTorch
    logger.info(f"\n{'='*80}")
    logger.info("STEP 1: Vanilla PyTorch (Ground Truth)")
    logger.info(f"{'='*80}")
    
    try:
        pytorch_logits, pytorch_metrics = test_vanilla_pytorch(model, tokenizer, test_prompts, device)
        logger.info("‚úÖ Step 1 completed")
    except Exception as e:
        logger.error(f"‚ùå Step 1 failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Test 2: Ring Buffer (optional)
    if args.skip_ring_buffer:
        logger.info("\n‚è≠Ô∏è  Skipping ring buffer test as requested")
        return True
    
    logger.info(f"\n{'='*80}")
    logger.info("STEP 2: Ring Buffer")
    logger.info(f"{'='*80}")
    
    try:
        ring_logits, ring_metrics = test_ring_buffer(model, tokenizer, test_prompts, device)
        
        if ring_logits is None:
            logger.error("‚ùå Step 2 failed: ring buffer test returned None")
            return False
        
        logger.info("‚úÖ Step 2 completed")
    except Exception as e:
        logger.error(f"‚ùå Step 2 failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Compare results
    logger.info(f"\n{'='*80}")
    logger.info("STEP 3: Compare Results")
    logger.info(f"{'='*80}")
    
    is_match, error_msg, metrics = compare_tensors(
        ring_logits,
        pytorch_logits,
        rtol=1e-2,
        atol=0.02,
        name="logits"
    )
    
    # Check predictions
    pytorch_preds = get_next_tokens(pytorch_logits, top_k=5)
    ring_preds = get_next_tokens(ring_logits, top_k=5)
    preds_match = torch.equal(pytorch_preds[:, 0], ring_preds[:, 0])  # Check top-1
    
    # Determine pass/fail
    passed = is_match or preds_match  # Pass if either criterion met
    
    logger.info(f"\n{'='*80}")
    logger.info("RESULTS")
    logger.info(f"{'='*80}")
    
    status = "‚úÖ PASS" if passed else "‚ùå FAIL"
    logger.info(f"{status}: Logit Equivalence Test")
    
    logger.info(f"\nComparison Metrics:")
    for key, value in metrics.items():
        logger.info(f"  {key}: {value:.6f}" if isinstance(value, float) else f"  {key}: {value}")
    
    logger.info(f"\nLogit Comparison: {error_msg}")
    
    logger.info(f"\nPrediction Comparison:")
    logger.info(f"  PyTorch top-5: {pytorch_preds[0].tolist()}")
    logger.info(f"  Ring buffer top-5: {ring_preds[0].tolist()}")
    logger.info(f"  Match (top-1): {'‚úÖ YES' if preds_match else '‚ùå NO'}")
    
    # Save results
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    results = {
        "test": "logit_equivalence",
        "model": args.model,
        "passed": passed,
        "is_match": is_match,
        "preds_match": preds_match.item() if isinstance(preds_match, torch.Tensor) else bool(preds_match),
        "comparison_metrics": metrics,
        "pytorch_metrics": pytorch_metrics,
        "ring_metrics": ring_metrics if ring_logits is not None else None,
        "timestamp": time.time(),
    }
    
    output_file = Path(args.output_dir) / "logit_equivalence_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"\nüìÑ Results saved to: {output_file}")
    
    return passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

