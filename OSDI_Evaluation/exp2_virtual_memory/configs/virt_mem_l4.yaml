# L4 GPU (24GB VRAM) Ring Buffer Configuration
# For Experiment 2: Virtual Memory via Skip-End Ring Buffer

experiment:
  name: "virtual_memory_ring_buffer_l4"
  description: "Experiment 2: Ring Buffer virtualization on L4 GPU (24GB)"
  
  # Ring buffer configuration
  ring_buffer:
    enabled: true
    capacity_gb: 16  # Use 16GB for weights on 24GB L4 (test virtualization with 13B model)
    prefetch_workers: 1  # Single worker sufficient on L4
  
  # Model configuration
  model:
    # For L4 (24GB VRAM):
    # - 6B models fit comfortably
    # - 7B models need ring buffer (~14GB FP16)
    # - 13B models need ring buffer (~26GB FP16, larger than VRAM)
    # - 70B models need ring buffer (~140GB FP16, 6x VRAM)
    model_id: "meta-llama/Llama-2-13b-hf"  # Slightly larger than VRAM to test streaming
    dtype: "float16"  # FP16 for bandwidth savings
    device: "cuda:0"
  
  # Inference configuration
  inference:
    batch_size: 1  # Single inference
    max_seq_len: 2048
    prompt_length: 512  # Initial prompt tokens
    generation_length: 50  # Tokens to generate
    temperature: 0.7
    top_p: 0.9
  
  # Measurement configuration
  measurement:
    runs: 5  # Number of inference runs for statistics
    warmup_runs: 2  # Warmup iterations (not measured)
    gc_between_runs: true  # Run garbage collection between runs
    disable_gc_during_run: true  # Disable GC during timed measurements
    ttft_enabled: true  # Measure Time-To-First-Token using model.generate()

# Baseline configurations (for comparison)
baselines:
  vllm:
    enabled: false  # May OOM on 24GB L4 with oversized models
    device_map: "sequential"
    max_num_seqs: 1
  
  hf_accelerate:
    enabled: true  # Baseline: HuggingFace Accelerate offloading
    device_map: "auto"
    offload_folder: "/tmp/hf_offload"
  
  deepspeed:
    enabled: false  # Optional: DeepSpeed baseline if installed
    config: "ds_config.json"
  
  ring_buffer_no_prefetch:
    enabled: false  # Ablation: ring buffer without async prefetch
    use_async_prefetch: false
  
  ring_buffer_async_prefetch:
    enabled: true  # Full Djinn implementation (main target)
    use_async_prefetch: true

# Performance tuning
tuning:
  # Host memory pinning
  pin_weights: true
  pin_memory_gb: 16  # Pre-allocate pinned memory pool
  
  # CUDA optimization
  cuda_graphs: false  # Disable for weight streaming
  torch_compile: false  # Disable for flexibility
  cudnn_benchmark: true  # Enable autotuner
  
  # Stream priorities
  prefetch_stream_priority: -1  # High priority (overrides compute stream)
  compute_stream_priority: 0   # Normal priority

# Evaluation environment (OSDI checklist)
environment:
  os_swap: "disabled"  # Should run: swapoff -a
  ulimit_memlock: "unlimited"  # Must run: ulimit -l unlimited
  numa_bind: false  # Optional: numactl --cpunodebind=X
  
  # GPU configuration
  gpu_index: 0  # Use first GPU
  max_gpu_memory_gb: 24  # Strict limit for L4
  os_reserve_gb: 1   # Reserve for OS
  safety_margin_gb: 2  # Safety margin (activations, temp buffers)

# Logging and output
logging:
  level: "INFO"
  verbose: false
  log_file: "/tmp/djinn_virt_mem_l4.log"

output:
  # Results saved to this directory
  results_dir: "OSDI_Evaluation/exp2_virtual_memory/results"
  
  # Metrics to collect
  metrics:
    - "effective_bandwidth_gbps"
    - "latency_ms"
    - "peak_vram_mb"
    - "logit_norm_diff"
    - "prefetch_success_rate"
    - "event_sync_latency_us"

