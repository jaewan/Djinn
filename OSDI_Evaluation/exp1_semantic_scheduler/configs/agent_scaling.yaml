experiment:
  name: "semantic_scheduler_agent_scaling"
  description: "Phase 3: Semantic Scheduler enables 50 concurrent agents via idle detection and KV swapping"
  version: "1.0"

workload:
  model_id: "meta-llama/Llama-2-7b-hf"
  dtype: "float16"
  agent_counts: [1, 2, 4, 8, 16, 32, 50]
  new_tokens: 50
  # sleep_seconds removed - using randomized uniform(8s, 12s) per Evaluation Plan
  iterations: 3

semantic_scheduler:
  enabled: true
  idle_threshold_seconds: 1.0
  host_swap_pool_gb: 32.0
  lifo_on_overload: true

server_config:
  enable_semantic_scheduler: true
  idle_threshold_seconds: 1.0
  host_swap_pool_gb: 32.0
  max_concurrent: 64
  max_vram_gb: 60.0

metrics:
  - name: "reason_latency_ms"
    description: "Prefill phase latency (first 50 tokens)"
  - name: "reflect_latency_ms"
    description: "Decode phase latency (includes KV restore)"
  - name: "p99_latency_ms"
    description: "P99 latency across all agents/phases"
  - name: "max_agents"
    description: "Maximum agents before OOM"
  - name: "throughput_agents_per_second"
    description: "Completion rate"
  - name: "swap_events"
    description: "Number of KV swaps performed"
  - name: "restore_latency_mean_ms"
    description: "Average KV restore overhead"

expected_results:
  agents_1:
    p99_latency_ms: 120
    success: true
  agents_2:
    p99_latency_ms: 130
    success: true
  agents_4:
    p99_latency_ms: 140
    success: true
  agents_8:
    p99_latency_ms: 150
    success: true
  agents_16:
    p99_latency_ms: 170
    success: true
  agents_32:
    p99_latency_ms: 200
    success: true
  agents_50:
    p99_latency_ms: 250
    success: true
    note: "Key result: Scales to 50 agents without OOM"

hardware_requirements:
  gpu: "NVIDIA A100-80GB"
  system_ram_gb: 128
  cpu_cores: 32
  network: "None (local)"

instructions:
  setup: |
    1. Start Djinn server with Phase 3:
       python -m djinn.server.server_main \
         --port 5556 \
         --gpu 0 \
         --enable-semantic-scheduler \
         --idle-threshold-seconds 1.0 \
         --host-swap-pool-gb 32 \
         --lifo-on-overload
    
    2. Download model:
       python - <<'EOF'
       from transformers import AutoModelForCausalLM, AutoTokenizer
       AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype="float16")
       AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
       EOF
  
  run: |
    python OSDI_Evaluation/exp1_semantic_scheduler/scripts/run_experiment.py \
      --config OSDI_Evaluation/exp1_semantic_scheduler/configs/agent_scaling.yaml \
      --model-id meta-llama/Llama-2-7b-hf \
      --output-dir OSDI_Evaluation/exp1_semantic_scheduler/results
  
  analyze: |
    python OSDI_Evaluation/exp1_semantic_scheduler/scripts/analyze_results.py \
      --results OSDI_Evaluation/exp1_semantic_scheduler/results/*.json \
      --output OSDI_Evaluation/exp1_semantic_scheduler/results/analysis.json

