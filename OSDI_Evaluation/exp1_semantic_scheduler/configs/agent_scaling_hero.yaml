experiment:
  name: "semantic_scheduler_hero"
  description: "HERO RESULT: N=80 with sustainable load (0.2 agents/sec)"
  version: "1.0"

workload:
  model_id: "meta-llama/Llama-2-7b-hf"
  dtype: "float16"
  total_agents: 80                     # Well above vLLM's OOM point (N=48)
  arrival_rate: 0.2                    # 1 agent per 5 seconds (sustainable)
  think_time_min: 10.0
  think_time_max: 20.0
  new_tokens: 50
  iterations: 1
  context_length: 1024

semantic_scheduler:
  enabled: true
  idle_threshold_seconds: 1.0
  host_swap_pool_gb: 32.0
  lifo_on_overload: true

server_config:
  enable_semantic_scheduler: true
  idle_threshold_seconds: 1.0
  host_swap_pool_gb: 32.0
  max_concurrent: 256

expected_results:
  total_duration_s: 500                # 80 agents @ 0.2/s = 400s arrival
  p99_wake_latency_ms: 500
  p99_request_latency_ms: 3000
  success_rate: 100
  swaps_gt: 100

