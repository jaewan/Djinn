\section{Introduction}

The economics of AI infrastructure are driving a fundamental architectural reversion: the return of the mainframe. With individual NVIDIA GB200 NVL72 racks commanding \$52–65 million—comparable to the inflation-adjusted cost of an IBM System/370—the era of the personal high-performance workstation has ended. This capital concentration necessitates a return to time-sharing, where thousands of users via "thin clients" (laptops) borrow compute from a centralized pool.

However, the software abstractions governing these clusters have failed to adapt to the dominant mode of AI development: \textit{interactive exploration}. Data scientists and researchers work in tight, human-speed loops—debugging model internals, testing hypotheses, and visualizing activations. 
Similarly, emerging Reasoning Agents exhibit identical patterns: they interleave bursty GPU generation with long-latency I/O (tool execution), leaving accelerators idle during these "machine think-times".
Unlike static production workloads, these tasks require the flexibility to execute arbitrary code combined with the elasticity to acquire resources in milliseconds.

Current infrastructure forces a bifurcation that leaves interactive workloads behind.
Production serving systems like vLLM~\cite{vllm} achieve high throughput via closed-loop execution: 
the system owns the execution loop, treating the model as a static appliance.
While they employ sophisticated paging, they rely on static execution graphs, lacking the semantic visibility to handle the arbitrary control flow, external tool usage, or state intervention required for research.
Conversely, cluster schedulers like Ray~\cite{ray} or Kubernetes~\cite{kubernetes} support arbitrary code but enforce coarse-grained batch semantics: users must reserve exclusive GPUs. 
This leads to the "parking lot" problem, where expensive accelerators sit idle while users think and type, resulting in average cluster utilization of 55--60\%~\cite{gao2024empirical}.

Enabling interactive exploration on shared accelerators requires more than simply disaggregating resources.
Existing disaggregation approaches~\cite{rcuda,dxpu,bitfusion} fail because they perform physical rather than logical virtualization.
As computation descends the software stack—from the deep learning framework, through the runtime, down to the driver and hardware—crucial semantic intent is stripped away~\cite{hong2025lost}.
Consequently, when a user pauses to "think," the remote GPU remains locked to hold their memory, solving nothing.
A PyTorch program understands that model weights are immutable and shared, while a KV-cache is session-private state. It knows when an operation is part of a long-lived prefill versus a short decode step. By the time these operations reach the driver, however, they have been flattened into opaque buffers and kernel launches. 
Lacking the high-level distinction between 'active computation' and 'interactive think-time', the driver is forced into a conservative posture: it must pin gigabytes of KV-cache in VRAM during minutes of user inactivity simply because the memory handle remains open. Similarly, without knowledge of model identity, it redundantly transfers identical weights for different users, unable to implement the shared-memory deduplication standard in modern OSes.

We argue that enabling efficient interactivity on shared accelerators requires a new abstraction: the \textbf{Tensor Process}.
Just as a Unix process virtualizes the CPU and RAM via Code, Heap, and Stack segments, a Tensor Process must virtualize the distinct semantic components of Deep Learning—Model Weights (Text), KV Caches (Data), and Activations (Stack).
This abstraction allows the system to manage memory lifecycles independently, enabling multiple sessions to time-share a single accelerator without thrashing.

We present \sys, a framework-level Tensor Operating System designed for this abstraction.
\sys bridges the semantic gap by interposing at the PyTorch dispatch layer, treating tensor operations as system calls.
It exposes a virtual tensor device to PyTorch: user code issues ordinary tensor operations against what appears to be a local accelerator, while \sys maps those operations and tensors onto remote GPU resources and physically multiplexes computations across a disaggregated cluster.
It introduces three OS primitives to the deep learning stack.

First, Lazy Evaluation via \textit{Semantically Rich Graph (SRG)}.
\sys acts as a semantic hypervisor, intercepting tensor operations and immediately returning a \textit{LazyTensor}—a virtual handle—without executing code or moving data.
This decouples the logical definition of computation from its physical execution. \sys records these operations into a \textit{Semantically Rich Graph (SRG)}, which captures tensor lifecycles and execution phases (e.g., prefill vs. decode). 
This allows the scheduler to optimize allocation globally, prefetching data for active phases and proactively swapping out idle sessions based on code structure.
The heavy lifting of graph capture, serialization, and remote execution is handled transparently, enabling features like \textit{Lazy I/O}, where \sys returns lightweight skeleton references and transfers concrete data only when the user explicitly inspects it.
%This graph drives a semantic scheduler that interleaves requests from multiple tenants, prioritizing interactive latency while maximizing throughput. SRG informs placement decisions: which tensors remain in the GPU and which are materialized or spilled, allowing \sys to co-locate long-lived state with compute. Additionally, the LazyTensor abstraction enables Lazy I/O: \sys returns lightweight "skeleton" references for complex results, transferring concrete data only when the user explicitly accesses it (e.g., printing a value).


Second, memory segmentation via \textit{Unified Virtual Memory Unit (VMU)}.
To realize Tensor Process, \sys partitions GPU memory into semantic segments defined by their lifecycle, mirroring Unix memory model.
Guided by the SRG, it maps immutable weights to a shared \textit{Text Segment}, session-private KV caches to a swappable \textit{Data Segment}, and ephemeral activations to a volatile \textit{Stack Segment}. 
Crucially, the Stack Segment employs a watermark-based reset, guaranteeing zero external fragmentation for intermediate computations.
This allows \sys to context-switch at the granularity of individual operations, solving the "parking lot" problem: users retain logical session state without locking physical compute.

Third, Latency-hiding Virtualization via \textit{Ghost Loading}. 
To virtualize access to massive models, \sys implements Ghost Loading. 
Clients instantiate zero-memory "ghost" modules; 
the server maps physical weights into the shared Text Segment.
Unlike general-purpose demand paging which reacts to page faults or relies on spatial locality heuristics, \sys leverages the SRG's deterministic lookahead to perfectly pipeline memory transfers with computation.
By treating the Text Segment as a streaming ring buffer, \sys prefetches future layers over PCIe synchronously while the GPU computes the current layer, effectively hiding virtualization latency."


Our evaluation shows that \sys enables new regimes of interactivity and efficiency. For reasoning agents, \sys eliminates the "parking lot" effect, scaling concurrent sessions by ×× over batch schedulers by transparently swapping state during tool execution. 
For heterogeneous pipelines, \sys runs multi-model workflows (e.g., Audio-LLM-Vision) on a single GPU that would otherwise OOM, by dynamically paging "Text Segments." \sys demonstrates that by lifting OS primitives into the framework layer, we can transform the GPU cluster from a rigid batch processor into a flexible interactive utility.


%Emerging \textit{AI Agents} exhibit identical structural sparsity but at higher frequency. Unlike stateless HTTP inference requests, Agents execute stateful loops: \textit{Reason → Act → Observe}.
%An agent might generate a plan on the GPU (Reason), execute a remote tool or database query (Act), and wait for the result (Observe).
%During the I/O-bound Act/Observe phase, the GPU execution units are idle, yet the VRAM must preserve the semantic context (the agent's KV history) for the next step.
%Current serverless abstractions fail here because they conflate idleness with termination. Releasing the GPU during I/O forces a full context flush, necessitating a prohibitively expensive Cold Start(reloading weights and re-processing history) at every step of the reasoning chain.

\paragraph{The Coupling Pathology.}
Both scenarios reveal a fundamental architectural inefficiency: the rigid coupling of Memory Capacity to Compute Bandwidth.
Under the current monolithic acquisition model, a process must reserve a static VRAM footprint to establish an execution context. Consequently, maintaining session state (residency) necessitates holding the accelerator's compute entitlement, even during idle periods.
This results in stranded compute capacity: the execution units are technically idle but practically inaccessible because available memory is exhausted by dormant state.
This forces a trade-off between utilization (batching) and interactivity (caching).
A Tensor Operating System resolves this by decoupling these resources, implementing distinct virtualization strategies: space-sharing VRAM to preserve hot session state via demand paging, while time-sharing execution units to multiplex sporadic compute bursts via sub-millisecond scheduling.

\subsection{The Semantic Translation Gap}

As operations descend the software stack, semantic intent is compiled into opaque resource transactions.

At the \textbf{Framework Layer} (e.g., PyTorch), a \textit{scaled\_dot\_product\_attention} call carries rich context: it identifies the operation type, distinguishes immutable weights from stateful KV-caches, and defines the DAG structure.

By the \textbf{Driver Layer} (e.g., CUDA), this context collapses. The driver sees a sequence of generic kernel launches and flat \textit{malloc} calls. It cannot distinguish a 12GB allocation representing shared, read-only model weights from a 12GB allocation representing ephemeral activations. Both are simply pointers.

At the \textbf{Hardware Layer} (PCIe/NVLink), context is erased. The system observes only DMA transactions defined by physical addresses.

This blindness forces lower-layer systems into conservative behaviors. Unable to identify which data is shareable (Weights) versus private (KV), a driver-level system must enforce strict process isolation. It cannot deduplicate weights across tenants, nor can it migrate an interactive session without copying the entire VRAM context---even if the active state is only a fraction of the allocated memory.

\subsection{The Cost of Agnosticism}

We quantify the impact of this semantic blindness across three representative workloads.

\paragraph{LLM Serving (Residency \& Locality).}
LLMs are defined by state management. Weights are immutable/shared; KV-caches are private/growing; activations are ephemeral. A driver-level system (e.g., rCUDA~\cite{rcuda}) sees only memory writes. To serve 50 users, it must allocate 50 copies of the weights, exhausting cluster memory. In contrast, a framework-aware system identifies the \textbf{Residency} of weights as "Global/Read-Only" and KV-caches as "Session/Private." In our benchmarks (GPT-J), this semantic caching reduces the memory footprint for 50 concurrent sessions by \jw{todo}.

\paragraph{Computer Vision (Scheduling).}
Convolutional networks exhibit strict sequential dataflow ($L_1 \rightarrow L_2 \rightarrow L_3$). A driver-level system sees independent kernels and forces sequential execution to ensure safety. Framework visibility exposes the dependency chain, allowing the system to automate \textbf{Scheduling} decisions like pipeline parallelism. \jw{Update to newer models}On ResNet-50, overlapping communication with computation reduces latency by \jw{todo}.

\paragraph{Recommendation Systems (Access Patterns).}
Embedding lookups follow a Zipfian distribution. A hardware-level system sees random access and defaults to LRU eviction. The framework layer observes embedding IDs, enabling value-based caching that pins "hot" rows. This semantic caching reduces average lookup latency by \jw{todo, maybe cut this}.

\subsection{The Design Space}

We evaluate four intervention points for implementing disaggregation (Table~\ref{tab:design_space}).

\paragraph{Hardware (PCIe-over-Fabric).}
Systems like DxPU \cite{dxpu} extend PCIe over the network. While transparent, they suffer from protocol impedance mismatch (PCIe assumes nanosecond latency) and total semantic blindness, precluding intelligent caching.

\paragraph{Driver (CUDA Interception).}
Systems like BitFusion \cite{bitfusion,rcuda} intercept GPU calls. This provides binary compatibility but suffers from "API Churn"---the maintenance burden of wrapping hundreds of evolving CUDA APIs. More critically, driver interception enforces process-level isolation, making fine-grained tensor sharing impossible.

\paragraph{Application (Manual Management).}
Specialized serving engines like vLLM \cite{vllm} or DistServe \cite{distserve} achieve high performance by manually partitioning specific models.
While efficient, this approach is Black-Box: it restricts users to a fixed API, prohibiting the arbitrary code execution required for interactive research. It violates the systems principle of generality.

\begin{table}[t]
\centering
\small
\caption{The Framework layer balances semantic visibility with application generality.}
\label{tab:design_space}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Layer} & \textbf{Semantic} & \textbf{Multi-} & \textbf{Stable} & \textbf{Zero} \\
& \textbf{Visibility} & \textbf{Tenancy} & \textbf{API} & \textbf{Code} \\ \midrule
Hardware & None & Low & Yes & Yes \\
Driver & Low & Low & No & Yes \\
Application & High & High & N/A & No \\
\textbf{Framework} & \textbf{High} & \textbf{High} & \textbf{Yes} & \textbf{Yes} \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Framework (The Semantic Narrow Waist).}
The ML framework acts as the "narrow waist" of the stack. It observes graph structures and tensor lifecycles (High Visibility) yet intercepts operations via standard dispatch mechanisms (High Generality). It abstracts hardware, allowing one codebase to support NVIDIA, AMD, or TPU backends. This is the optimal layer for a Tensor OS.


\subsection{Design Requirements}

To realize a Tensor Operating System at the framework layer, \sys targets five specific requirements:

\begin{itemize}
    \item \textbf{Transparent Capture:} Infer semantic intent (phases, lifecycles) from standard PyTorch code without manual user annotation.
    \item \textbf{Virtualization:} Decouple logical tensors from physical memory. Data should never move to the client unless explicitly accessed ("Zero Data Movement").
    \item \textbf{Zero-Fragmentation:} Manage remote memory to eliminate external fragmentation while enforcing session isolation (addressing the Residency decision).
    \item \textbf{Failure Resilience:} Detect and recover from remote failures at the tensor operation boundary.
    \item \textbf{Interactive Latency:} Mask network latency via lookahead scheduling to approach local execution speeds for interactive use.
\end{itemize}

Existing systems fail at least two of these. \sys satisfies all five.

\section{Evaluation}
\label{sec:evaluation}

We evaluate \sys's ability to virtualize GPU memory through a controlled oversubscription experiment. Our goal is to verify that semantic phase signaling enables the system to support concurrent interactive workloads that exceed physical device capacity, a regime where reactive baselines fail.

\subsection{Memory Oversubscription and Agent Density}
\label{ssec:eval_density}
\jw{main finished, ready for review. Will add another line of Djinn real remote. Current plot runs Djinn in local cilent -> server via local network for apple to apple comparison with vLLM running locally. will add my personal laptop -> server (real Djinn model). }

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/cliff.pdf}
    \caption{\textbf{The Scalability Cliff.} vLLM (red) crashes when aggregate KV demand exceeds physical memory ($N \approx 48$). \sys (blue) successfully virtualizes memory, maintaining linear queueing latency well into the oversubscription regime ($N=80$, demand=106GB), demonstrating effective use of host DRAM for active state.\jw{need to rerun vLLM, it is worse than shown}}
    \label{fig:cliff}
\end{figure}

To test the limits of virtualization, we instantiate a multi-agent workload designed to force paging. We spawn $N=80$ concurrent agents executing a \textit{Reason $\to$ Act $\to$ Reflect} loop driven by a Poisson arrival process ($\lambda=0.2$ agents/s) to model asynchronous interactivity. Each agent manages a 2,048-token context using Llama-2-13B (26 GB weights in FP16).

The aggregate memory demand of this workload is 106 GB ($80$ agents $\times$ $1$ GB KV + $26$ GB weights), which exceeds the 80 GB capacity of our NVIDIA H100 testbed. Consequently, successful execution is not merely a performance optimization but a proof of virtualization: without active swapping to host memory, the workload cannot reside in physical memory.

We compare \sys against vLLM, configured with its native PagedAttention swapping enabled. As shown in Figure~\ref{fig:cliff}, vLLM handles concurrency effectively until $N \approx 48$. Beyond this point, its reactive policy fails to reclaim memory fast enough during bursty arrivals, resulting in cascading OOM failures and a $0\%$ success rate for $N \ge 50$. vLLM holds inactive KV caches in VRAM until memory pressure is critical; by the time it evicts, the allocation overhead of new agents has already triggered exhaustion.

In contrast, \sys leverages semantic signals (\texttt{IO\_WAIT}) emitted during the agent's ``Act'' phase. 
While we explicitly inject these system calls in our experiment to isolate scheduler performance, \sys provides library-level wrappers (e.g., for LangChain Tool classes) that automatically emit these signals upon entering I/O-bound blocks, rendering the mechanism transparent to the end user.
Modeled as 10--20s of I/O dormancy, these phases expose idle intervale that \sys exploits for proactive eviction. By moving the session's data segment immediately to the server's pinned host DRAM pool (provisioned from 512 GB system RAM), \sys successfully interleaves all 80 agents with a $100\%$ completion rate. This represents a $1.67\times$ improvement in feasible agent density over the reactive baseline, directly converting host RAM capacity into GPU concurrency.

\subsection{Latency and Overhead Decomposition}
\jw{Used to achieve 9 seconds p99. Will try to revert it by the deadline.}
While \sys enables higher density, we must characterize the latency cost of this virtualization. At $N=80$, the P99 request latency is 23.9 seconds. Decomposing this figure reveals that the system overhead is negligible compared to the queuing delay inherent to high utilization:

\begin{equation}
\label{eq:latency}
T_{total} (23.9\text{s}) = T_{queue} (22.2\text{s}) + T_{transfer} (0.05\text{s}) + T_{compute} (1.7\text{s})
\end{equation}

The semantic swap-in mechanism ($T_{transfer}$) accounts for only $0.2\%$ of the total latency ($\sim 50$ms), saturating the PCIe Gen4 bus at $\sim 24$ GB/s. The dominant component ($92.7\%$) is $T_{queue}$, the time requests spend waiting for a compute slot. This queueing is an expected artifact of fair scheduling: with 80 active sessions time-sharing a single accelerator, the GPU maintains near-100\% utilization, processing 3--4 agents simultaneously while others wait.

Crucially, this latency profile confirms that the \sys runtime is not the bottleneck. The system effectively virtualizes the memory hierarchy, trading latency (queue wait) for capacity (density) without introducing significant virtualization overhead. Unlike sequential baselines which would serialize 80 agents, forcing the 80th user to endure a $\sim$112s Time-to-First-Token latency ($80 \times 1.4$s per inference), \sys provides fair, concurrent progress for all users, maintaining the illusion of a dedicated (albeit slower) device for workloads that physically exceed the hardware.

\subsection{Memory Virtualization and Fractional Residency}
\label{sec:eval_virt}

While \S\ref{ssec:eval_density} demonstrated \sys's ability to virtualize memory for many small workloads, Experiment 2 evaluates the complementary challenge: virtualizing a single massive workload that exceeds physical device capacity. This scenario is critical for commodity "Edge AI" clusters where model size often outstrips the VRAM of affordable accelerators (e.g., NVIDIA L4).

We test the hypothesis that \sys's architectural support for Fractional Residency—keeping a maximal working set resident while streaming only the non-resident delta—fundamentally outperforms the Binary Offloading (all-GPU or all-CPU) policies employed by state-of-the-art serving systems.

We conduct a controlled oversubscription experiment using a single NVIDIA L4 GPU (24 GB VRAM).
\begin{itemize}
    \item \textbf{Workload:} We execute inference on Llama-2-13B (FP16), which requires 26 GB for parameters. This exceeds the L4's capacity by 2 GB ($1.08\times$ oversubscription), forcing the system to virtualize memory.
    \item \textbf{Baseline:} We compare against DeepSpeed-Inference \cite{deepspeed} configured with ZeRO-Offload. DeepSpeed represents the industry standard for offloading; when VRAM is exhausted, it manages execution by swapping parameters from host RAM.
    \item \textbf{Configuration:} To ensure a fair comparison, both systems execute the same cuBLAS compute kernels. The divergence lies strictly in I/O architecture: DeepSpeed employs synchronous, blocking transfers, whereas \sys employs a Ring Buffer with async pipelining.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/exp2_memory.pdf}
    \caption{\textbf{Time-to-First-Token (TTFT) Latency Breakdown.} 
    \textbf{Left (Baseline):} DeepSpeed employs a binary offloading policy. Because the 26~GB model exceeds the 24~GB VRAM, it synchronously reloads the full model checkpoint from host RAM, blocking execution for 35.5s (red hatched).
    \textbf{Right (\sys):} \sys employs fractional residency via the Ring Buffer. It pins 20~GB in VRAM and streams only the 6~GB non-resident delta. By reducing data movement by $4\times$ and pipelining the transfer behind computation (cyan dotted), \sys reduces prefill latency by \textbf{31.4$\times$} (36.2s $\to$ 1.1s). Note the broken y-axis used to visualize the magnitude of the baseline's stall.}
    \label{fig:virt_speedup}
\end{figure}

Figure~\ref{fig:virt_speedup} presents the latency breakdown for the prefill phase (Time-to-First-Token, TTFT). \sys reduces TTFT from 36.2 seconds to 1.1 seconds—a 31.4$\times$ speedup.

This performance gap is architectural. DeepSpeed's execution model enforces a binary choice: because the 26 GB model does not fit entirely in VRAM, the allocator defaults to a swap-based strategy that reloads the full model checkpoint for every inference request. This incurs a blocking transfer of 24.2 GB, idling the GPU for 35.5 seconds before computation can begin.
DeepSpeed's high latency reflects not just PCIe transfer time, but the overhead of CPU-side tensor copy, pinned memory allocation, and serialization inherent to its offloading implementation, which is not optimized for the 'fractional' regime.

In contrast, \sys's Ring Buffer maintains \textit{Fractional Residency}. It pins 20 GB of the model (77\%) in VRAM and identifies only the 6 GB non-resident delta. During the prefill phase, \sys streams this 6 GB delta at the limit of the PCIe Gen4 x16 bus ($\sim$15 GB/s effective).
While the initial fill of the ring buffer prevents perfect overlap, the virtualization cost is reduced from the time required to load the \textit{entire model} ($T_{load} \approx 36\text{s}$) to the time required to stream the \textit{delta} ($T_{stream} \approx 0.4\text{s}$). When combined with the 0.7s compute time, this yields a total TTFT of 1.1s.

\subsubsection{The Physics of Decoding}
While \sys dominates the prefill phase, our evaluation reveals the physical limits of virtualization during the autoregressive decode phase.
As shown in Table~\ref{tab:virt_metrics}, both DeepSpeed and \sys converge to a per-token generation latency of $\sim$704ms.

\begin{table}[h]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{DeepSpeed} & \textbf{\sys (Ours)} \\
\midrule
Time-to-First-Token (TTFT) & 36.2 s & \textbf{1.1 s} \\
Decode Latency (per token) & 704 ms & 705 ms \\
End-to-End (50 tokens) & 71.4 s & \textbf{36.4 s} \\
\midrule
Data Moved (per inference) & 24.2 GB & \textbf{6.0 GB} \\
I/O Model & Blocking & \textbf{Pipelined} \\
\bottomrule
\end{tabular}
\caption{\textbf{Virtualization Microbenchmarks.} \sys achieves a 31$\times$ improvement in responsiveness (TTFT) by moving $4\times$ less data. Decode latency is identical between systems, as throughput is strictly limited by the PCIe bandwidth required to stream the non-resident weights per token.}
\label{tab:virt_metrics}
\end{table}

This parity is expected. During decoding, computation is memory-bound and sequential. To generate a single token, the system must process the non-resident 6 GB of weights. Given the PCIe bandwidth limit of 15 GB/s, the theoretical minimum transfer time is $6 \text{ GB} / 15 \text{ GB/s} = 400 \text{ ms}$.
The measured 704ms latency implies roughly 300ms of compute/kernel overhead, consistent with the 700ms compute time observed in the prefill phase.
Combined with kernel overhead ($\sim$300ms), the physical floor is $\sim$700ms. Neither system can bypass this hardware bottleneck.

The significance of \sys is not that it makes the hardware faster, but that it makes the hardware \textit{usable}.
DeepSpeed's 36-second startup latency renders the L4 GPU unusable for interactive applications; users perceive the system as frozen. \sys's 1.1-second TTFT falls within the interactive response threshold.
By converting a hard capacity wall into a linear bandwidth cost, \sys's Ring Buffer transforms commodity GPUs from batch-only processors into viable interactive agents, achieving a 2.0$\times$ end-to-end speedup for short sequences while reducing aggregate PCIe traffic by 4$\times$.
