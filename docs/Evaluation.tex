\section{Evaluation}
\label{sec:evaluation}

We evaluate \sys on three axes: (1) multi-model switching reliability and latency, (2) memory virtualization for models exceeding GPU capacity, and (3) white-box interactivity for debugging workflows. Our testbed comprises NVIDIA H100 (80GB) and L4 (24GB) GPUs.

\subsection{Multi-Model Switching}
\label{ssec:eval_multimodel}

We deploy three 7B-class models (Llama-2-7B at 12.6GB, Mistral-7B at 13.5GB, Phi-2 at 5.2GB) on an H100 with a 25GB ring buffer---insufficient to hold all three simultaneously (31.3GB total). We issue 30 sequential requests with Zipfian model selection, forcing continuous eviction and restoration. Each request generates 50 tokens. We execute requests \textit{sequentially} to isolate switching mechanics from scheduler queuing effects. We compare against four baselines: (1) \textbf{vLLM}, the state-of-the-art serving engine, which must destroy and recreate its engine on model switch; (2) \textbf{PyTorch Pinned}, which loads weights from page-locked host memory via \texttt{.to('cuda')}---the fair baseline since \sys also uses pinned memory; (3) \textbf{PyTorch Pageable}, standard \texttt{.to('cuda')} from pageable memory; and (4) \textbf{Serverless}, emulating cold-start by loading from disk per request.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/latency_cdf.pdf}
    \caption{\textbf{Multi-Model Switching Latency CDF.} \sys (blue) exhibits a trimodal distribution: 53\% of requests complete in $<$10ms (cache hits), 40\% in 1--4s (normal swaps), and 7\% in $>$10s (cold starts). PyTorch Pinned (green) shows no caching---all requests pay 4.3s transfer cost. vLLM (red) fails 27\% of requests (horizontal ceiling at 73\%) due to memory fragmentation; surviving requests pay 18.2s engine recreation overhead.}
    \label{fig:multimodel_cdf}
\end{figure}

\paragraph{Results.} Figure~\ref{fig:multimodel_cdf} reveals the fundamental architectural differences between systems. \sys achieves a median latency of \textbf{1,392ms}---3.1$\times$ lower than PyTorch Pinned (4,302ms) and 13.1$\times$ lower than vLLM (18,213ms). This speedup stems from ring buffer residency: \textbf{16 out of 30 requests (53\%)} incur switch overhead below 10ms because the target model remains resident from a prior request. The CDF shows this as a sharp step at the 10ms mark. An additional 12 requests (40\%) complete in 1--4 seconds, representing normal model swaps where the system loads weights from pinned host memory at PCIe bandwidth (15 GB/s). Only 2 requests (7\%) experience cold-start penalties exceeding 10 seconds, occurring when the ring buffer must evict 19GB of resident models to accommodate a new one.

In contrast, PyTorch Pinned exhibits a vertical wall at 4.3s in the CDF, confirming that it lacks any residency tracking---every request pays the full 12--14GB PCIe transfer cost. PyTorch Pageable performs even worse (P50: 10,317ms) due to the overhead of transferring from non-pinned memory. Serverless emulation (P50: 4,062ms) demonstrates that loading from disk is comparable to PyTorch Pinned's memory transfer, but neither approach benefits from model reuse.

vLLM's failure is architectural: it completes only 22 of 30 requests (73\% success rate), with the CDF terminating at the 73rd percentile. The 8 failed requests (27\%) crash due to GPU memory fragmentation from repeated engine destruction and recreation. For surviving requests, vLLM's median latency of 18,213ms reflects the cost of full engine initialization, including CUDA context setup and memory allocation. This is 13.1$\times$ slower than \sys and proves that reactive serving systems are fundamentally unsuitable for multi-model workloads.

\paragraph{The Cost of Virtualization.} \sys's P99 latency (14,905ms) is 2.8$\times$ higher than PyTorch Pinned (5,402ms), exposing the tail cost of memory virtualization. This worst-case occurs when Agent 1 requests llama-7b, triggering the eviction of 19GB of resident models (mistral-7b and phi-2). The system must transfer these models to host memory (9.7s) before loading llama-7b (5.2s). Critically, this is a \textit{one-time} cold-start penalty: subsequent requests for llama-7b complete in $<$10ms. The mean latency of 2,522ms (vs 3,912ms for PyTorch Pinned) confirms that cold starts are rare and their cost amortizes across model reuse. More importantly, \sys converts what would be an OOM crash in a capacity-constrained system into bounded latency, maintaining 100\% availability.

\subsection{Memory Virtualization and Fractional Residency}
\label{ssec:eval_virt}

\S\ref{ssec:eval_multimodel} virtualized multi-model residency; here we virtualize capacity. We target the \textit{boundary condition}: a workload that marginally exceeds physical memory.We run Llama-2-13B (24.2~GB FP16) on an NVIDIA L4 (24~GB), creating $1.01\times$ oversubscription.This regime isolates allocation policy. Does the system treat memory as binary (fit/crash) or continuous (fit/degrade)?
%While massive oversubscription (e.g., $6\times$) would trivially bind all systems to PCIe bandwidth, the $1.01\times$ regime isolates the \textit{allocation policy}. It tests whether the system treats memory as a binary constraint or a continuous resource.
We compare against GPU-Only (standard \texttt{model.cuda()}), DeepSpeed-Inference~\cite{deepspeed} (industry-standard optimized runtime with kernel injection), and HuggingFace Accelerate (\texttt{device\_map="auto"}, standard CPU offloading).


\begin{table}[t]
\centering
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Baseline} & \textbf{TTFT} & \textbf{Decode} & \textbf{E2E (50 tok)} \\
\midrule
GPU-Only & \multicolumn{3}{c}{OOM (binary failure)} \\
DeepSpeed & \multicolumn{3}{c}{OOM (binary failure)} \\
HF Accelerate & 4,365 ms & 4,254 ms/tok & 212.7 s \\
\textbf{\sys} & \textbf{297 ms} & \textbf{113 ms/tok} & \textbf{5.6 s} \\
\midrule
\textbf{Speedup} & \textbf{14.7$\times$} & \textbf{37.7$\times$} & \textbf{37.7$\times$} \\
\bottomrule
\end{tabular}
\caption{\textbf{Memory Virtualization (Llama-2-13B on L4).} Existing runtimes crash at $1.01\times$ oversubscription. \sys converts capacity overflow into latency cost, achieving 14.7--37.7$\times$ speedup over the only working baseline.}
\label{tab:virt_metrics}
\end{table}

~\cref{tab:virt_metrics} shows the results. Both GPU-Only and DeepSpeed fail with OOM. DeepSpeed's \texttt{init\_inference} attempts full GPU residency during kernel injection---a $1\%$ capacity overflow causes $100\%$ availability loss. This validates that even optimized runtimes enforce binary residency; they do not virtualize capacity.
Against HuggingFace Accelerate (the only surviving baseline), \sys achieves 14.7$\times$ faster TTFT (297ms vs 4.4s) and 37.7$\times$ faster decode (113ms vs 4.3s/tok).
The gap is architectural. HuggingFace employs synchronous layer-wise offloading: each of 40 layers blocks on PCIe, and every decode step re-transfers CPU-resident weights. At 15~GB/s, even ideal transfer takes 1.6s; Python overhead adds 2.8s more.

\sys implements fractional residency. It pins 32 blocks (17.7~GB, 73\%) in a 20~GB ring buffer and streams 11 blocks (6.5~GB, 27\%) through 3 rotating slots. Two-ahead pipelining (prefetch block $N{+}2$ while executing $N$) fully hides transfer latency.
As a result, decode becomes compute-bound, not I/O-bound. The 113ms/token approaches native GPU speed despite oversubscription.

\sys transforms a 24~GB GPU from ``unusable for 24.2~GB models'' to near-native performance. The 297ms TTFT is below the 500ms threshold for perceived instantaneity; 50-token generation takes 5.6s instead of 3.5 minutes.
Fractional residency converts a hard capacity wall into a soft latency tax, making commodity GPUs viable for models that nominally exceed their memory—and, critically, making disaggregation robust to the heterogeneous capacity demands of thin-client populations.

\subsection{White-Box Interactivity and State Virtualization}
\label{ssec:eval_interactivity}

While \S\ref{ssec:eval_multimodel} demonstrated virtualization for reliability, interactive research requires inspection and intervention. We evaluate \sys's ability to expose mutable process state through a ``Breakpoint \& Steering'' experiment, quantifying the cost of virtualizing active sessions.

\paragraph{Workload and Oversubscription.}
We configure a single NVIDIA H100 (80 GB) to serve Llama-2-13B. To force virtualization, we instantiate $N=50$ concurrent interactive sessions. Each session performs a forward pass, triggers a \textit{semantic breakpoint} at Layer 20, inspects activations on the CPU, and resumes execution.
This workload creates an aggregate memory demand that strictly exceeds physical capacity:
\begin{equation}
\label{eq:interactivity_math}
M_{total} = \underbrace{27\text{ GB}}_{\text{Shared Weights}} + \sum_{i=1}^{50} (\underbrace{1.3\text{ GB}}_{\text{Private KV}_i}) \approx 92\text{ GB}
\end{equation}
Since $M_{total} > M_{physical}$ ($92 > 80$), the workload is unsatisfiable without paging the private state of paused sessions.

\paragraph{Baseline Analysis: The Parking Lot Problem.}
We compare \sys against a standard PyTorch Eager baseline. In the baseline, a paused session retains ownership of its private KV cache and activation scratchpads in VRAM. Our measurements confirm that a single paused PyTorch session holds 24.3 GB of VRAM. Consequently, the H100 saturates at $N=3$; the fourth concurrent session triggers an OOM crash. This illustrates the ``Parking Lot'' pathology: the baseline treats the accelerator as a blocking resource, serializing interactive users and degrading cluster utilization to $<10\%$.

\paragraph{\sys Results: Concurrency and Overhead.}
\sys executes all $N=50$ sessions with a 100\% success rate.
Upon reaching the Layer 20 breakpoint, the \sys runtime detects the transition to the \texttt{IO\_WAIT} phase and pages the session's 1.3 GB private data segment to pinned host memory.
We quantify virtualization overhead as the latency penalty perceived by the resuming user, $T_{overhead} = T_{dispatch} + T_{restore}$.
Because \sys employs an asynchronous command queue, client-side dispatch ($T_{dispatch}$) is negligible ($<10 \mu$s). Physical state restoration ($T_{restore}$) is dominated by PCIe transfer time. Moving the 1.3 GB state over PCIe Gen4 incurs a latency of $\sim20$ ms. This mechanism enables a $16\times$ increase in concurrent capacity ($N=50$ vs $N=3$) with sub-frame latency penalties.

\begin{table}[t]
\centering
\small
\caption{\textbf{Interactivity Microbenchmarks.} Comparison of resource efficiency and capabilities on Llama-2-13B (H100). \sys transforms the GPU from a blocking processor into a time-shared utility, enabling 50 concurrent debugging sessions where the baseline fails at 3.}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{PyTorch Eager} & \textbf{\sys (Ours)} \\
\midrule
\textit{Resource Efficiency} & & \\
VRAM Held (Paused) & 24.3 GB & \textbf{$\approx$ 0 GB} \\
Max Concurrent Sessions & 3 & \textbf{50+} \\
Oversubscription & Crash & \textbf{Virtualize} \\
\midrule
\textit{Interactive Latency} & & \\
Breakpoint Dispatch & N/A & \textbf{$<10 \mu$s} \\
State Restore (PCIe) & N/A & \textbf{$\sim 20$ ms} \\
\midrule
\textit{Capabilities} & & \\
State Access & Read-Only & \textbf{Read/Write} \\
Steering Divergence & N/A & \textbf{0.39\%} \\
\bottomrule
\end{tabular}
\label{tab:interactivity}
\end{table}

\paragraph{Verification of Write-Back Semantics.}
To verify that \sys enables genuine intervention (write access) rather than passive inspection, we perform an \textit{activation steering} test. At the Layer 20 breakpoint, we inject a perturbation vector (scaling activations by $0.9\times$) into the virtualized state on the host before resuming execution on the device. We measure a deterministic token output divergence of 0.39\% compared to a golden reference run.
This confirms that \sys correctly propagates state modifications through the virtualization layer. This capability is architecturally distinct from compiled serving engines (e.g., vLLM), which treat internal KV state as opaque and immutable, thereby precluding white-box research workflows.

\subsection{Resume Latency: The Scalability of State Restoration}
\label{ssec:eval_resume_latency}

The central promise of \sys is that semantic virtualization imposes only a constant-time ($O(1)$) latency penalty, decoupling debugging cost from model depth. To validate this, we measure the end-to-end \textit{Resume Latency}—the wall-clock time from a user issuing a ``Resume'' command at layer $L$ to the GPU beginning execution of layer $L+1$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/resume_latency.pdf}
    \caption{\textbf{Resume Latency Scaling.} Recompute (red) scales linearly ($O(L)$), taking $>65$ms at Layer 40, which degrades interactivity. Manual Offload (green) is optimal ($O(1)$, 0.8ms) but requires manual memory management. \sys (blue) achieves $O(1)$ scalability ($\sim$35ms) transparently, decoupling debug cost from model depth. The gap between \sys and Manual Offload is strictly framework implementation overhead (Python RPC), not architectural.}
    \label{fig:resume_latency}
\end{figure}

We compare three resumption strategies on Llama-2-13B (H100):
\begin{itemize}
    \item \textbf{Stateless Recompute:} Discard intermediate state at breakpoints. On resume, re-execute the model from Layer 0. This is the default behavior of stateless serving systems.
    \item \textbf{Manual Offload (Oracle):} Explicitly pin activation tensors to host RAM and copy them back via \texttt{cudaMemcpyAsync}. This represents the theoretical hardware lower bound (``speed of light'') for PCIe interconnects but requires intrusive user code.
    \item \textbf{\sys Resume:} Transparently restore state from the semantic memory manager via RPC.
\end{itemize}

\paragraph{The O(L) vs. O(1) Crossover.}
Figure~\ref{fig:resume_latency} reveals the fundamental scaling limitation of stateless approaches. Recomputation exhibits linear $O(L)$ scaling: while fast at Layer 1 (2.1 ms), it grows to \textbf{65.4 ms} by Layer 40. For deep debugging sessions involving repeated stepping, this latency accumulates, creating perceptible lag and reducing debug loop frequency to $<15$ Hz.

In contrast, \sys maintains a constant-time ($O(1)$) latency profile of $\sim$35 ms regardless of breakpoint depth. While slower than recompute for shallow layers ($L < 20$), \sys becomes the superior strategy as model depth increases, guaranteeing predictable interactivity even for massive models where recomputation would take seconds.

\paragraph{Decomposing the Transparency Tax.}
While \sys achieves $O(1)$ scalability, it incurs a latency overhead compared to the Manual Offload oracle (0.8 ms). We decompose this $\sim$34 ms gap to isolate architectural costs from implementation artifacts:
\begin{equation}
T_{\sys} \approx \underbrace{20\text{ ms}}_{\text{Framework}} + \underbrace{10\text{ ms}}_{\text{Semantic Logic}} + \underbrace{5\text{ ms}}_{\text{GPU}}
\end{equation}
The dominant cost (20 ms) is \textbf{Framework Overhead} attributable to our prototype's use of Python \texttt{asyncio} and TCP for control plane RPCs, rather than an optimized C++ transport (e.g., gRPC/RDMA) which typically yields $<2$ ms latency \cite{ray,pytorch}. The actual \textbf{Semantic Logic}—the architectural cost of looking up and scheduling the session—adds only $\sim$10 ms.
This decomposition confirms that the latency gap is not a fundamental limitation of disaggregation but an implementation choice. Even with this prototype overhead, \sys remains well within the 100 ms threshold for human-perceived instantaneous response, successfully trading minimal latency for the profound benefit of transparent, code-free state virtualization.
