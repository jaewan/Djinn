\section{Evaluation}
\label{sec:evaluation}

We evaluate \sys on three axes: (1) multi-model switching reliability and latency, (2) memory virtualization for models exceeding GPU capacity, and (3) white-box interactivity for debugging workflows. Our testbed comprises NVIDIA H100 (80GB) and L4 (24GB) GPUs.

\subsection{Multi-Model Switching}
\label{ssec:eval_multimodel}

We deploy three 7B-class models (Llama-2-7B at 12.6GB, Mistral-7B at 13.5GB, Phi-2 at 5.2GB) on an H100 with a 25GB ring buffer---insufficient to hold all three simultaneously (31.3GB total). We issue 30 sequential requests with Zipfian model selection, forcing continuous eviction and restoration. Each request generates 50 tokens. We execute requests \textit{sequentially} to isolate switching mechanics from scheduler queuing effects. We compare against four baselines: (1) \textbf{vLLM}, the state-of-the-art serving engine, which must destroy and recreate its engine on model switch; (2) \textbf{PyTorch Pinned}, which loads weights from page-locked host memory via \texttt{.to('cuda')}---the fair baseline since \sys also uses pinned memory; (3) \textbf{PyTorch Pageable}, standard \texttt{.to('cuda')} from pageable memory; and (4) \textbf{Serverless}, emulating cold-start by loading from disk per request.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/latency_cdf.pdf}
    \caption{\textbf{Multi-Model Switching Latency CDF.} \sys (blue) exhibits a trimodal distribution: 53\% of requests complete in $<$10ms (cache hits), 40\% in 1--4s (normal swaps), and 7\% in $>$10s (cold starts). PyTorch Pinned (green) shows no caching---all requests pay 4.3s transfer cost. vLLM (red) fails 27\% of requests (horizontal ceiling at 73\%) due to memory fragmentation; surviving requests pay 18.2s engine recreation overhead.}
    \label{fig:multimodel_cdf}
\end{figure}

\paragraph{Results.} Figure~\ref{fig:multimodel_cdf} reveals the fundamental architectural differences between systems. \sys achieves a median latency of \textbf{1,392ms}---3.1$\times$ lower than PyTorch Pinned (4,302ms) and 13.1$\times$ lower than vLLM (18,213ms). This speedup stems from ring buffer residency: \textbf{16 out of 30 requests (53\%)} incur switch overhead below 10ms because the target model remains resident from a prior request. The CDF shows this as a sharp step at the 10ms mark. An additional 12 requests (40\%) complete in 1--4 seconds, representing normal model swaps where the system loads weights from pinned host memory at PCIe bandwidth (15 GB/s). Only 2 requests (7\%) experience cold-start penalties exceeding 10 seconds, occurring when the ring buffer must evict 19GB of resident models to accommodate a new one.

In contrast, PyTorch Pinned exhibits a vertical wall at 4.3s in the CDF, confirming that it lacks any residency tracking---every request pays the full 12--14GB PCIe transfer cost. PyTorch Pageable performs even worse (P50: 10,317ms) due to the overhead of transferring from non-pinned memory. Serverless emulation (P50: 4,062ms) demonstrates that loading from disk is comparable to PyTorch Pinned's memory transfer, but neither approach benefits from model reuse.

vLLM's failure is architectural: it completes only 22 of 30 requests (73\% success rate), with the CDF terminating at the 73rd percentile. The 8 failed requests (27\%) crash due to GPU memory fragmentation from repeated engine destruction and recreation. For surviving requests, vLLM's median latency of 18,213ms reflects the cost of full engine initialization, including CUDA context setup and memory allocation. This is 13.1$\times$ slower than \sys and proves that reactive serving systems are fundamentally unsuitable for multi-model workloads.

\paragraph{The Cost of Virtualization.} \sys's P99 latency (14,905ms) is 2.8$\times$ higher than PyTorch Pinned (5,402ms), exposing the tail cost of memory virtualization. This worst-case occurs when Agent 1 requests llama-7b, triggering the eviction of 19GB of resident models (mistral-7b and phi-2). The system must transfer these models to host memory (9.7s) before loading llama-7b (5.2s). Critically, this is a \textit{one-time} cold-start penalty: subsequent requests for llama-7b complete in $<$10ms. The mean latency of 2,522ms (vs 3,912ms for PyTorch Pinned) confirms that cold starts are rare and their cost amortizes across model reuse. More importantly, \sys converts what would be an OOM crash in a capacity-constrained system into bounded latency, maintaining 100\% availability.

\subsection{Memory Virtualization and Fractional Residency}
\label{ssec:eval_virt}

\S\ref{ssec:eval_multimodel} virtualized multi-model residency; here we virtualize capacity. We target the \textit{boundary condition}: a workload that marginally exceeds physical memory.We run Llama-2-13B (24.2~GB FP16) on an NVIDIA L4 (24~GB), creating $1.01\times$ oversubscription.This regime isolates allocation policy. Does the system treat memory as binary (fit/crash) or continuous (fit/degrade)?
%While massive oversubscription (e.g., $6\times$) would trivially bind all systems to PCIe bandwidth, the $1.01\times$ regime isolates the \textit{allocation policy}. It tests whether the system treats memory as a binary constraint or a continuous resource.
We compare against GPU-Only (standard \texttt{model.cuda()}), DeepSpeed-Inference~\cite{deepspeed} (industry-standard optimized runtime with kernel injection), and HuggingFace Accelerate (\texttt{device\_map="auto"}, standard CPU offloading).


\begin{table}[t]
\centering
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Baseline} & \textbf{TTFT} & \textbf{Decode} & \textbf{E2E (50 tok)} \\
\midrule
GPU-Only & \multicolumn{3}{c}{OOM (binary failure)} \\
DeepSpeed & \multicolumn{3}{c}{OOM (binary failure)} \\
HF Accelerate & 4,365 ms & 4,254 ms/tok & 212.7 s \\
\textbf{\sys} & \textbf{297 ms} & \textbf{113 ms/tok} & \textbf{5.6 s} \\
\midrule
\textbf{Speedup} & \textbf{14.7$\times$} & \textbf{37.7$\times$} & \textbf{37.7$\times$} \\
\bottomrule
\end{tabular}
\caption{\textbf{Memory Virtualization (Llama-2-13B on L4).} Existing runtimes crash at $1.01\times$ oversubscription. \sys converts capacity overflow into latency cost, achieving 14.7--37.7$\times$ speedup over the only working baseline.}
\label{tab:virt_metrics}
\end{table}

~\cref{tab:virt_metrics} shows the results. Both GPU-Only and DeepSpeed fail with OOM. DeepSpeed's \texttt{init\_inference} attempts full GPU residency during kernel injection---a $1\%$ capacity overflow causes $100\%$ availability loss. This validates that even optimized runtimes enforce binary residency; they do not virtualize capacity.
Against HuggingFace Accelerate (the only surviving baseline), \sys achieves 14.7$\times$ faster TTFT (297ms vs 4.4s) and 37.7$\times$ faster decode (113ms vs 4.3s/tok).
The gap is architectural. HuggingFace employs synchronous layer-wise offloading: each of 40 layers blocks on PCIe, and every decode step re-transfers CPU-resident weights. At 15~GB/s, even ideal transfer takes 1.6s; Python overhead adds 2.8s more.

\sys implements fractional residency. It pins 32 blocks (17.7~GB, 73\%) in a 20~GB ring buffer and streams 11 blocks (6.5~GB, 27\%) through 3 rotating slots. Two-ahead pipelining (prefetch block $N{+}2$ while executing $N$) fully hides transfer latency.
As a result, decode becomes compute-bound, not I/O-bound. The 113ms/token approaches native GPU speed despite oversubscription.

\sys transforms a 24~GB GPU from ``unusable for 24.2~GB models'' to near-native performance. The 297ms TTFT is below the 500ms threshold for perceived instantaneity; 50-token generation takes 5.6s instead of 3.5 minutes.
Fractional residency converts a hard capacity wall into a soft latency tax, making commodity GPUs viable for models that nominally exceed their memoryâ€”and, critically, making disaggregation robust to the heterogeneous capacity demands of thin-client populations.

\subsection{White-box Interactivity \& State Virtualization}
\label{ssec:eval_interactivity}

To validate \sys's ability to virtualize execution state for interactive debugging---a capability absent in monolithic serving systems like vLLM---we conduct a controlled experiment measuring \textit{Resume Latency}. We define Resume Latency as the wall-clock time from issuing a \texttt{resume()} command for a paused session to receiving the computation result.

We instrument an Llama-2-13B inference session on an NVIDIA H100 GPU (80GB). We execute a 2048-token sequence, triggering a breakpoint at varying depths (Layers 1, 10, 20, 30, and 39). Each measurement reports the mean over 5 runs after 2 warmup iterations. We compare \sys against two baselines:
\begin{enumerate}
    \item \textbf{Stateless Recompute (Baseline):} The standard "serverless" approach. On pause, all state is discarded. On resume, the system recomputes activations from the input tokens up to the target layer. This scales as $O(L)$, where $L$ is the layer depth.
    \item \textbf{Manual Offload (Oracle):} A theoretical lower bound representing a "hand-optimized" script. This baseline keeps the CPU process alive and manually moves the activation tensor (10.5MB) from pinned CPU memory to GPU using explicit \texttt{cudaMemcpy}. It measures only the hardware data transfer time ($O(1)$), ignoring all scheduling and safety overheads.
\end{enumerate}
\sys is configured to transparently persist activations in the virtualization layer, requiring no user code changes.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/resume_latency.pdf}
    \caption{\textbf{Resume Latency vs. Breakpoint Depth.} Recompute (red) scales linearly ($O(L)$), degrading from 2.1ms to 64.4ms. Manual Offload (green) is constant at 0.8ms but requires manual memory management. \sys (blue) maintains bounded latency (5.6--66.5ms) and outperforms Recompute by Layer 30, demonstrating that virtualization is performant for deep-layer debugging. The decreasing \sys latency reflects diminishing compute tail ($N-L$ remaining layers), not growing overhead. The shaded region highlights the crossover where \sys becomes faster than stateless recomputation.}
    \label{fig:resume_latency}
\end{figure}

Figure~\ref{fig:resume_latency} presents the Resume Latency across depths. Unlike Recompute, which degrades linearly with depth (2.1ms at Layer 1 $\rightarrow$ 64.4ms at Layer 39), \sys's total latency remains bounded (5.6--66.5ms) and becomes strictly faster than Recompute by Layer 30 (20.0ms vs 49.2ms). This confirms that for deep-layer debugging tasks---the common case in interactive workflows---virtualization outperforms stateless recomputation. The measured latency includes both checkpoint restoration (constant $O(1)$ at $\sim$10.5MB activation transfer) and execution of the remaining $N-L$ layers; the decreasing trend reflects the diminishing compute tail, not growing virtualization overhead.

Manual Offload achieves a hardware-limit latency of 0.8ms (pure PCIe DMA). \sys incurs 5.6--66.5ms, a gap of 4.8--65.7ms representing the cost of the OS abstraction: session management, RPC serialization, and safety checks. Critically, the \textit{control-plane overhead} (measured as client-server latency delta) decreases from 32ms at Layer 1 to 1.8ms at Layer 39. The 1.8ms floor represents the pure framework cost (Python asyncio + TCP), while the larger gap at shallow layers includes the synchronous wait for GPU completion of the remaining layers. Across all layers, \sys maintains latencies well below the 100ms human perception threshold (max 66.5ms at Layer 1), confirming that the overhead, while mathematically significant compared to the Oracle, is negligible for the target interactive use case.

The unique value of \sys is not raw speed---manual \texttt{cudaMemcpy} will always be faster---but \textit{State Virtualization}. Manual Offload tightly couples execution to a specific OS process and GPU memory address; if the client disconnects or the process terminates, the session is lost. \sys decouples the session state from the client process, enabling persistence, migration, and multi-tenant arbitration (\S\ref{ssec:eval_virt}). The measured overhead is an architectural trade-off: we accept 5--35ms of latency (median 20ms at Layer 30) to transform a brittle, single-process execution into a robust, OS-managed service. The 1.8ms control-plane floor at Layer 39 demonstrates that the core virtualization mechanism is lightweight; future implementations using C++-based RPC (e.g., gRPC) and asynchronous handles could further reduce this overhead to sub-millisecond levels, approaching the theoretical limit while preserving transparency.