\section{Evaluation}
\label{sec:evaluation}
\jw{Exp 1,2,3 are the main experiments I planned. Will add more baselines to each experiments and optionally another ablation study if time allows.}

We evaluate \sys's ability to virtualize GPU memory through a controlled oversubscription experiment. Our goal is to verify that semantic phase signaling enables the system to support concurrent interactive workloads that exceed physical device capacity, a regime where reactive baselines fail.

\subsection{Memory Oversubscription and Agent Density}
\label{ssec:eval_density}
\jw{main finished, ready for review. Will add another line of Djinn real remote. Current plot runs Djinn in local cilent -> server via local network for apple to apple comparison with vLLM running locally. will add my personal laptop -> server (real Djinn model). }

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/cliff.pdf}
    \caption{\textbf{The Scalability Cliff.} vLLM (red) crashes when aggregate KV demand exceeds physical memory ($N \approx 48$). \sys (blue) successfully virtualizes memory, maintaining linear queueing latency well into the oversubscription regime ($N=80$, demand=106GB), demonstrating effective use of host DRAM for active state.\jw{need to rerun vLLM, it is worse than shown}}
    \label{fig:cliff}
\end{figure}

To test the limits of virtualization, we instantiate a multi-agent workload designed to force paging. We spawn $N=80$ concurrent agents executing a \textit{Reason $\to$ Act $\to$ Reflect} loop driven by a Poisson arrival process ($\lambda=0.2$ agents/s) to model asynchronous interactivity. Each agent manages a 2,048-token context using Llama-2-13B (26 GB weights in FP16).

The aggregate memory demand of this workload is 106 GB ($80$ agents $\times$ $1$ GB KV + $26$ GB weights), which exceeds the 80 GB capacity of our NVIDIA H100 testbed. Consequently, successful execution is not merely a performance optimization but a proof of virtualization: without active swapping to host memory, the workload cannot reside in physical memory.

We compare \sys against vLLM, configured with its native PagedAttention swapping enabled. As shown in Figure~\ref{fig:cliff}, vLLM handles concurrency effectively until $N \approx 48$. Beyond this point, its reactive policy fails to reclaim memory fast enough during bursty arrivals, resulting in cascading OOM failures and a $0\%$ success rate for $N \ge 50$. vLLM holds inactive KV caches in VRAM until memory pressure is critical; by the time it evicts, the allocation overhead of new agents has already triggered exhaustion.

In contrast, \sys leverages semantic signals (\texttt{IO\_WAIT}) emitted during the agent's ``Act'' phase. 
While we explicitly inject these system calls in our experiment to isolate scheduler performance, \sys provides library-level wrappers (e.g., for LangChain Tool classes) that automatically emit these signals upon entering I/O-bound blocks, rendering the mechanism transparent to the end user.
Modeled as 10--20s of I/O dormancy, these phases expose idle intervale that \sys exploits for proactive eviction. By moving the session's data segment immediately to the server's pinned host DRAM pool (provisioned from 512 GB system RAM), \sys successfully interleaves all 80 agents with a $100\%$ completion rate. This represents a $1.67\times$ improvement in feasible agent density over the reactive baseline, directly converting host RAM capacity into GPU concurrency.

\subsection{Latency and Overhead Decomposition}
\jw{Used to achieve 9 seconds p99. Will try to revert it by the deadline.}
While \sys enables higher density, we must characterize the latency cost of this virtualization. At $N=80$, the P99 request latency is 23.9 seconds. Decomposing this figure reveals that the system overhead is negligible compared to the queuing delay inherent to high utilization:

\begin{equation}
\label{eq:latency}
T_{total} (23.9\text{s}) = T_{queue} (22.2\text{s}) + T_{transfer} (0.05\text{s}) + T_{compute} (1.7\text{s})
\end{equation}

The semantic swap-in mechanism ($T_{transfer}$) accounts for only $0.2\%$ of the total latency ($\sim 50$ms), saturating the PCIe Gen4 bus at $\sim 24$ GB/s. The dominant component ($92.7\%$) is $T_{queue}$, the time requests spend waiting for a compute slot. This queueing is an expected artifact of fair scheduling: with 80 active sessions time-sharing a single accelerator, the GPU maintains near-100\% utilization, processing 3--4 agents simultaneously while others wait.

Crucially, this latency profile confirms that the \sys runtime is not the bottleneck. The system effectively virtualizes the memory hierarchy, trading latency (queue wait) for capacity (density) without introducing significant virtualization overhead. Unlike sequential baselines which would serialize 80 agents, forcing the 80th user to endure a $\sim$112s Time-to-First-Token latency ($80 \times 1.4$s per inference), \sys provides fair, concurrent progress for all users, maintaining the illusion of a dedicated (albeit slower) device for workloads that physically exceed the hardware.

\subsection{Memory Virtualization and Fractional Residency}
\label{ssec:eval_virt}

While \S\ref{ssec:eval_density} demonstrated \sys's ability to virtualize memory for many small workloads, Experiment 2 evaluates the complementary challenge: virtualizing a single massive workload that exceeds physical device capacity. This scenario is critical for commodity "Edge AI" clusters where model size often outstrips the VRAM of affordable accelerators (e.g., NVIDIA L4).

We test the hypothesis that \sys's architectural support for Fractional Residency—keeping a maximal working set resident while streaming only the non-resident delta—fundamentally outperforms the Binary Offloading (all-GPU or all-CPU) policies employed by state-of-the-art serving systems.

We conduct a controlled oversubscription experiment using a single NVIDIA L4 GPU (24 GB VRAM).
\begin{itemize}
    \item \textbf{Workload:} We execute inference on Llama-2-13B (FP16), which requires 26 GB for parameters. This exceeds the L4's capacity by 2 GB ($1.08\times$ oversubscription), forcing the system to virtualize memory.
    \item \textbf{Baseline:} We compare against DeepSpeed-Inference \cite{deepspeed} configured with ZeRO-Offload. DeepSpeed represents the industry standard for offloading; when VRAM is exhausted, it manages execution by swapping parameters from host RAM.
    \item \textbf{Configuration:} To ensure a fair comparison, both systems execute the same cuBLAS compute kernels. The divergence lies strictly in I/O architecture: DeepSpeed employs synchronous, blocking transfers, whereas \sys employs a Ring Buffer with async pipelining.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/exp2_memory.pdf}
    \caption{\textbf{Time-to-First-Token (TTFT) Latency Breakdown.} 
    \textbf{Left (Baseline):} DeepSpeed employs a binary offloading policy. Because the 26~GB model exceeds the 24~GB VRAM, it synchronously reloads the full model checkpoint from host RAM, blocking execution for 35.5s (red hatched).
    \textbf{Right (\sys):} \sys employs fractional residency via the Ring Buffer. It pins 20~GB in VRAM and streams only the 6~GB non-resident delta. By reducing data movement by $4\times$ and pipelining the transfer behind computation (cyan dotted), \sys reduces prefill latency by \textbf{31.4$\times$} (36.2s $\to$ 1.1s). Note the broken y-axis used to visualize the magnitude of the baseline's stall.}
    \label{fig:virt_speedup}
\end{figure}

Figure~\ref{fig:virt_speedup} presents the latency breakdown for the prefill phase (Time-to-First-Token, TTFT). \sys reduces TTFT from 36.2 seconds to 1.1 seconds—a 31.4$\times$ speedup.

This performance gap is architectural. DeepSpeed's execution model enforces a binary choice: because the 26 GB model does not fit entirely in VRAM, the allocator defaults to a swap-based strategy that reloads the full model checkpoint for every inference request. This incurs a blocking transfer of 24.2 GB, idling the GPU for 35.5 seconds before computation can begin.
DeepSpeed's high latency reflects not just PCIe transfer time, but the overhead of CPU-side tensor copy, pinned memory allocation, and serialization inherent to its offloading implementation, which is not optimized for the 'fractional' regime.

In contrast, \sys's Ring Buffer maintains \textit{Fractional Residency}. It pins 20 GB of the model (77\%) in VRAM and identifies only the 6 GB non-resident delta. During the prefill phase, \sys streams this 6 GB delta at the limit of the PCIe Gen4 x16 bus ($\sim$15 GB/s effective).
While the initial fill of the ring buffer prevents perfect overlap, the virtualization cost is reduced from the time required to load the \textit{entire model} ($T_{load} \approx 36\text{s}$) to the time required to stream the \textit{delta} ($T_{stream} \approx 0.4\text{s}$). When combined with the 0.7s compute time, this yields a total TTFT of 1.1s.

\subsubsection{The Physics of Decoding}
While \sys dominates the prefill phase, our evaluation reveals the physical limits of virtualization during the autoregressive decode phase.
As shown in Table~\ref{tab:virt_metrics}, both DeepSpeed and \sys converge to a per-token generation latency of $\sim$704ms.

\begin{table}[h]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{DeepSpeed} & \textbf{\sys (Ours)} \\
\midrule
Time-to-First-Token (TTFT) & 36.2 s & \textbf{1.1 s} \\
Decode Latency (per token) & 704 ms & 705 ms \\
End-to-End (50 tokens) & 71.4 s & \textbf{36.4 s} \\
\midrule
Data Moved (per inference) & 24.2 GB & \textbf{6.0 GB} \\
I/O Model & Blocking & \textbf{Pipelined} \\
\bottomrule
\end{tabular}
\caption{\textbf{Virtualization Microbenchmarks.} \sys achieves a 31$\times$ improvement in responsiveness (TTFT) by moving $4\times$ less data. Decode latency is identical between systems, as throughput is strictly limited by the PCIe bandwidth required to stream the non-resident weights per token.}
\label{tab:virt_metrics}
\end{table}

This parity is expected. During decoding, computation is memory-bound and sequential. To generate a single token, the system must process the non-resident 6 GB of weights. Given the PCIe bandwidth limit of 15 GB/s, the theoretical minimum transfer time is $6 \text{ GB} / 15 \text{ GB/s} = 400 \text{ ms}$.
The measured 704ms latency implies roughly 300ms of compute/kernel overhead, consistent with the 700ms compute time observed in the prefill phase.
Combined with kernel overhead ($\sim$300ms), the physical floor is $\sim$700ms. Neither system can bypass this hardware bottleneck.

The significance of \sys is not that it makes the hardware faster, but that it makes the hardware \textit{usable}.
DeepSpeed's 36-second startup latency renders the L4 GPU unusable for interactive applications; users perceive the system as frozen. \sys's 1.1-second TTFT falls within the interactive response threshold.
By converting a hard capacity wall into a linear bandwidth cost, \sys's Ring Buffer transforms commodity GPUs from batch-only processors into viable interactive agents, achieving a 2.0$\times$ end-to-end speedup for short sequences while reducing aggregate PCIe traffic by 4$\times$.

\subsection{White-Box Interactivity and State Virtualization}
\label{ssec:eval_interactivity}

While \S\ref{ssec:eval_density} demonstrated virtualization for throughput, interactive research requires inspection and intervention. We evaluate \sys's ability to expose mutable process state through a ``Breakpoint \& Steering'' experiment, quantifying the cost of virtualizing active sessions.

\paragraph{Workload and Oversubscription.}
We configure a single NVIDIA H100 (80 GB) to serve Llama-2-13B. To force virtualization, we instantiate $N=50$ concurrent interactive sessions. Each session performs a forward pass, triggers a \textit{semantic breakpoint} at Layer 20, inspects activations on the CPU, and resumes execution.
This workload creates an aggregate memory demand that strictly exceeds physical capacity:
\begin{equation}
\label{eq:interactivity_math}
M_{total} = \underbrace{27\text{ GB}}_{\text{Shared Weights}} + \sum_{i=1}^{50} (\underbrace{1.3\text{ GB}}_{\text{Private KV}_i}) \approx 92\text{ GB}
\end{equation}
Since $M_{total} > M_{physical}$ ($92 > 80$), the workload is unsatisfiable without paging the private state of paused sessions.

\paragraph{Baseline Analysis: The Parking Lot Problem.}
We compare \sys against a standard PyTorch Eager baseline. In the baseline, a paused session retains ownership of its private KV cache and activation scratchpads in VRAM. Our measurements confirm that a single paused PyTorch session holds 24.3 GB of VRAM. Consequently, the H100 saturates at $N=3$; the fourth concurrent session triggers an OOM crash. This illustrates the ``Parking Lot'' pathology: the baseline treats the accelerator as a blocking resource, serializing interactive users and degrading cluster utilization to $<10\%$.

\paragraph{\sys Results: Concurrency and Overhead.}
\sys executes all $N=50$ sessions with a 100\% success rate.
Upon reaching the Layer 20 breakpoint, the \sys runtime detects the transition to the \texttt{IO\_WAIT} phase and pages the session's 1.3 GB private data segment to pinned host memory.
We quantify virtualization overhead as the latency penalty perceived by the resuming user, $T_{overhead} = T_{dispatch} + T_{restore}$.
Because \sys employs an asynchronous command queue, client-side dispatch ($T_{dispatch}$) is negligible ($<10 \mu$s). Physical state restoration ($T_{restore}$) is dominated by PCIe transfer time. Moving the 1.3 GB state over PCIe Gen4 incurs a latency of $\sim20$ ms. This mechanism enables a $16\times$ increase in concurrent capacity ($N=50$ vs $N=3$) with sub-frame latency penalties.

\begin{table}[t]
\centering
\small
\caption{\textbf{Interactivity Microbenchmarks.} Comparison of resource efficiency and capabilities on Llama-2-13B (H100). \sys transforms the GPU from a blocking processor into a time-shared utility, enabling 50 concurrent debugging sessions where the baseline fails at 3.}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{PyTorch Eager} & \textbf{\sys (Ours)} \\
\midrule
\textit{Resource Efficiency} & & \\
VRAM Held (Paused) & 24.3 GB & \textbf{$\approx$ 0 GB} \\
Max Concurrent Sessions & 3 & \textbf{50+} \\
Oversubscription & Crash & \textbf{Virtualize} \\
\midrule
\textit{Interactive Latency} & & \\
Breakpoint Dispatch & N/A & \textbf{$<10 \mu$s} \\
State Restore (PCIe) & N/A & \textbf{$\sim 20$ ms} \\
\midrule
\textit{Capabilities} & & \\
State Access & Read-Only & \textbf{Read/Write} \\
Steering Divergence & N/A & \textbf{0.39\%} \\
\bottomrule
\end{tabular}
\label{tab:interactivity}
\end{table}

\paragraph{Verification of Write-Back Semantics.}
To verify that \sys enables genuine intervention (write access) rather than passive inspection, we perform an \textit{activation steering} test. At the Layer 20 breakpoint, we inject a perturbation vector (scaling activations by $0.9\times$) into the virtualized state on the host before resuming execution on the device. We measure a deterministic token output divergence of 0.39\% compared to a golden reference run.
This confirms that \sys correctly propagates state modifications through the virtualization layer. This capability is architecturally distinct from compiled serving engines (e.g., vLLM), which treat internal KV state as opaque and immutable, thereby precluding white-box research workflows.

\subsection{Resume Latency: The Scalability of State Restoration}
\label{ssec:eval_resume_latency}

The central promise of \sys is that semantic virtualization imposes only a constant-time ($O(1)$) latency penalty, decoupling debugging cost from model depth. To validate this, we measure the end-to-end \textit{Resume Latency}—the wall-clock time from a user issuing a ``Resume'' command at layer $L$ to the GPU beginning execution of layer $L+1$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/resume_latency.pdf}
    \caption{\textbf{Resume Latency Scaling.} Recompute (red) scales linearly ($O(L)$), taking $>65$ms at Layer 40, which degrades interactivity. Manual Offload (green) is optimal ($O(1)$, 0.8ms) but requires manual memory management. \sys (blue) achieves $O(1)$ scalability ($\sim$35ms) transparently, decoupling debug cost from model depth. The gap between \sys and Manual Offload is strictly framework implementation overhead (Python RPC), not architectural.}
    \label{fig:resume_latency}
\end{figure}

We compare three resumption strategies on Llama-2-13B (H100):
\begin{itemize}
    \item \textbf{Stateless Recompute:} Discard intermediate state at breakpoints. On resume, re-execute the model from Layer 0. This is the default behavior of stateless serving systems.
    \item \textbf{Manual Offload (Oracle):} Explicitly pin activation tensors to host RAM and copy them back via \texttt{cudaMemcpyAsync}. This represents the theoretical hardware lower bound (``speed of light'') for PCIe interconnects but requires intrusive user code.
    \item \textbf{\sys Resume:} Transparently restore state from the semantic memory manager via RPC.
\end{itemize}

\paragraph{The O(L) vs. O(1) Crossover.}
Figure~\ref{fig:resume_latency} reveals the fundamental scaling limitation of stateless approaches. Recomputation exhibits linear $O(L)$ scaling: while fast at Layer 1 (2.1 ms), it grows to \textbf{65.4 ms} by Layer 40. For deep debugging sessions involving repeated stepping, this latency accumulates, creating perceptible lag and reducing debug loop frequency to $<15$ Hz.

In contrast, \sys maintains a constant-time ($O(1)$) latency profile of $\sim$35 ms regardless of breakpoint depth. While slower than recompute for shallow layers ($L < 20$), \sys becomes the superior strategy as model depth increases, guaranteeing predictable interactivity even for massive models where recomputation would take seconds.

\paragraph{Decomposing the Transparency Tax.}
While \sys achieves $O(1)$ scalability, it incurs a latency overhead compared to the Manual Offload oracle (0.8 ms). We decompose this $\sim$34 ms gap to isolate architectural costs from implementation artifacts:
\begin{equation}
T_{\sys} \approx \underbrace{20\text{ ms}}_{\text{Framework}} + \underbrace{10\text{ ms}}_{\text{Semantic Logic}} + \underbrace{5\text{ ms}}_{\text{GPU}}
\end{equation}
The dominant cost (20 ms) is \textbf{Framework Overhead} attributable to our prototype's use of Python \texttt{asyncio} and TCP for control plane RPCs, rather than an optimized C++ transport (e.g., gRPC/RDMA) which typically yields $<2$ ms latency \cite{ray,pytorch}. The actual \textbf{Semantic Logic}—the architectural cost of looking up and scheduling the session—adds only $\sim$10 ms.
This decomposition confirms that the latency gap is not a fundamental limitation of disaggregation but an implementation choice. Even with this prototype overhead, \sys remains well within the 100 ms threshold for human-perceived instantaneous response, successfully trading minimal latency for the profound benefit of transparent, code-free state virtualization.
